<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Text to Image">
    <meta name="author" content="MALL lab">

    <title>KVQA</title>

    <link href="kvqa_ProjectFiles/bootstrap.css" rel="stylesheet">
    <link href="kvqa_ProjectFiles/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
      <script src="js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="container">
      <div class="header">
        <h3 class="text"><center>KVQA: Knowledge-aware Visual Question Answering</center></h3>
       </div>

 <div class="container">
      <div class="header">
      <br>
<center>
<img src="kvqa_ProjectFiles/goal1.png" height="500" border="1">
&nbsp;
&nbsp;
&nbsp;
</center>
Q: Who is to the left of Barack Obama? <br>
A: <font color="green">Richard Cordray</font><br>
Q: Do people in the image have common occupation? <br>
A: <font color="green">Yes</font><br>
Q: Who among the people in the image is called by the nickname Barry?<br>
A: <font color="green">Person in the center</font>
</div>



      <div class="row">
        <h3>Motivation</h3>
        <p style="text-align: justify;">                   
Visual Question Answering (VQA) has emerged as an important
problem spanning Computer Vision, Natural Language
Processing and Artificial Intelligence (AI). In conventional
VQA, one may ask questions about an image which can be
answered purely based on its content. For example, given
an image with people in it, a typical VQA question may
inquire about the number of people in the image. More recently,
there is growing interest in answering questions which
require commonsense knowledge involving common nouns
(e.g., cats, dogs, microphones) present in the image. In spite
of this progress, the important problem of answering questions
requiring world knowledge about named entities (e.g.,
Barack Obama, White House, United Nations) in the image
has not been addressed in prior research. We address this
gap in this paper, and introduce KVQA â€“ the first dataset
for the task of (world) knowledge-aware VQA. KVQA consists
of 183K question-answer pairs involving more than 18K
named entities and 24K images. Questions in this dataset
require multi-entity, multi-relation, and multi-hop reasoning
over large Knowledge Graphs (KG) to arrive at an answer. To
the best of our knowledge, KVQA is the largest dataset for
exploring VQA over KG. Further, we also provide baseline
performances using state-of-the-art methods on KVQA.
        </p>
      </div>
      <div class="row">
        <h3>Highlights</h3>
     <ul> 
      <li> The largest dataset for VQA over KG (as on November 14, 2018)</li>
     <li> An important but unexplored problem of VQA involving named entities in an image. </li>
     <li> Visual entity linking problem in web-scale </li>
     <li> Challenges for Computer Vision: face identification at web-scale </li>
     <li> Challenges for NLP: reasoning over KG </li>s
     </ul>
               
     </div>


<div class="row">
         <h3 id="datasetE">Dataset: Explore</h3>
    <p>&nbsp;</p>
<table style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td><img style="margin-right: 10px; float: left;" src="kvqa_ProjectFiles/sample.png" alt="I4" height="190" /></td>
</tr>
</tbody>
</table>
<a href=""> Explore more </a>

<div class="row">
         <h3 id="datasetD">Dataset: Downloads</h3>
        
            <div class="row">
<p><a href="">KVQA1.0 - dataset images</a> (XX GB)<br>
<p><a href="">KVQA1.0 - reference images</a> (XX GB)<br>
<p><a href="">KVQA1.0 - Other supporting files</a> (XX GB)<br>
<br>
<a href="README.txt">README</a><br>
<a href="update.txt">Updates</a></p>
<hr>


      <div class="row">
         <h3 id="code"> Baseline models and code </h3>
           Coming soon .. 
           


<h3><strong><span style="font-size: 12pt;">Bibtex</span></strong></h3>
<p>If you use this dataset, please cite:</p>
<pre><tt>@InProceedings{shahMYP18,
  author    = "Shah, S., Mishra, A., Yadati, N. and Talukdar, P.~P.",
  title     = "KVQA: Knowledge-Aware Visual Question Answering",
  booktitle = "AAAI",
  year      = "2019",
}</tt></pre>
<hr>

        <h3>Publications</h3>
       <br>
Sanket Shah*, Anand Mishra*, Naganand Yadati, Partha Pratim Talukdar, <b>KVQA: Knowledge-Aware Visual Question Answering</b>, AAAI 2019
[<a href=""><u>pdf</u></a>][<a href=""><u>Abstract</u></a>][<a href="kvqa_ProjectFiles/KVQA-AAAI19Spotlight_v2.pdf
"><u>Spotlight slides</u></a>] (*: Equal contributions)

<br>
<br>


            
 <div class="row">
       <h3>People</h3>
        <a href="https://anandmishra22.github.io/"><u>Anand Mishra</u></a> <br>
        <a href="http://talukdar.net/"><u>Partha Talukdar</u></a><br>
        <a href=""> <u>Sanket Shah</u></a> <br>
        <a href=""> <u>Naganand Yadati</u></a> <br>
        
      </div>


      <div class="row">
       <h3>Acknowledgements</h3>
        <p> Authors would like to thank MHRD, Govt. of India and Intel Corporation for partly supporting this work. 
        </p>
      </div>
      
      

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
  


</div></div></body></html>
